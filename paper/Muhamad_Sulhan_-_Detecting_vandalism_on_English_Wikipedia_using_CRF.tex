\include{preamble}

\begin{document}

\input{title_authors}

\input{abstract}

\section{Introduction}
\label{section:introduction}
	\input{introduction}


\section{Related Works}
	\input{related_works}


\section{Literature Study}
	\label{section:literature_study}

This section review the previous work and techniques used for implementation in
this paper.

\subsection{LNSMOTE}
	\label{subsection:lnsmote}
	\input{lnsmote}

\subsection{Cascaded Random Forest}
	\label{subsection:crf}
	\input{crf}


\section{Vandalism Detection Process}
	\label{section:research_methodology}

This section describe the process to generate the feature dataset and
implementation of resampling and classifiers, started from data preparation,
generating features, resampling features dataset, and implementation so it can
be used on training, testing, and analysis.
Each step on implementation process is illustrated as in figure
\ref{fig:proses}.

\input{diagram_process}

\subsection{Data Preparation}
	\label{subsection:data_preparation}
	\input{data_preparation}

\subsection{Extracting Features}
	\input{extracting_features}

\subsubsection{Metadata Features}
	\input{metadata_features}

\subsubsection{Text Features}
	\input{text_features}

\subsubsection{Language Features}
	\input{language_features}

\subsection{Resampling Dataset}
	\input{resampling}


\subsection{Classifier Implementations}

Implementation of classifiers carried out gradually.
Started by implementing CART which is used in Random Forest, which in turn is
used in Cascaded Random Forest.
The implementation of CART based on Jiawei Han et al. book, chapter 8
\cite{han2011data}.
The implementation of Random Forest is based on original paper of Breiman
\cite{breiman2001random}, plus additional resource from internet.
The implementation of Cascaded Random Forest is based on original paper of
Baumann et al.
\cite{baumann2013cascaded}.
The result of all implementation is published as open source software to help
others in future research or for real-world usage.
\footnote{\url{https://github.com/shuLhan/go-mining/tree/master/classifiers}}.


\subsection{Training and Testing}

There are three common parameter between RF and CRF which are 200 for number of
tree, 5 for number of random features, and 64\% for percentage of
bootstrapping.
For consistency, their value are constant between training.
For CRF classifier, three separated testing will be conducted using different
parameter for number of stage and number of tree which are 200 stages with 1
tree, 100 stages with 2 trees, and 50 stages with 4 trees; all of them have
equal total number of trees.
This is an experiment to see the effect of number of trees to stage and their
performance.
Another parameter for training with CRF are thresholds for true-positive rate
(TPR) and true-negative rate (TNR), which set to constant value 0.95 and 0.95
for all training.

\input{table_dataset}

The dataset used for training is PAN-WVC-10 which consist of three different
set, dataset without resampled, dataset resampled with SMOTE, and dataset
resampled with LNSMOTE.
The dataset used for testing is PAN-WVC-11 which contain 1,143 positive
samples and 8,842 negative samples, in total of 9985 samples.

Training is conducted by running each classifier program, RF and CRF, on
those three different PAN-WVC-10 feature dataset.
Testing is conducted after the model has been built by giving the model the
PAN-WVC-11 feature dataset as an input.

The environment used for training and testing is Intel\textregistered
Core\texttrademark i7-4750HQ CPU 2,00 GHz, with total 8 GB of RAM.
Each training is done separatedly to avoid cache miss which affect computation
time.


\section{Evaluation}
	\label{section:result_and_analysis}
	\input{evaluation}

\section{Conclusion}
\label{section:conclusion}

On average SMOTE increase TPR value by $0.19$ times while LNSMOTE, on average
increase TPR value $0.33$ times.
Another interisting effect of CRF classifier, when using less number of tree on
each stage on dataset without resampling, their performance almost similar with
CRF on resampling with more number of tree, for example performance of CRF with
100 stages and 2 tress on dataset without resampling is adjacent with CRF with
50 stages and 4 tress on dataset resampled with SMOTE.

The best classifier model for vandalism without resampling returned by CRF with
200 stages and 1 tree with TPR value $0.9668$.
The best classifier model for dataset that has been resampled with SMOTE is CRF
with 200 stage and 1 trees with TPR value $0.979$
The best classifier model for dataset that has been resampled with LNSMOTE is
CRF with 200 stages and 1 trees with TPR value $0.9904$.
Overall, the best model is CRF with 200 stages and 1 trees on dataset resampled
with LNSMOTE.
Beside their good performance result, CRF on average $1.6$ times faster at
training than RF on resampled dataset.

\section{Contribution}

This paper contribute on finding the best classifier on detecting vandalism on
Wikipedia and evaluating the effect of LNSMOTE resampling on imbalance dataset
and performance of CRF agains RF.
Apart from that, this paper also provides a framework to create and develop
vandalism features from raw PAN WVC dataset without having to build it again
from scratch.
Another contribution is a library for data processing and machine learning,
especially on resampling using LNSMOTE and CRF classifier which has no
open implementation on renowned program like Weka, Scikit-Learn, or R.
This framework can be used in the next research or in real-world application.

\section{Future Works}

All of training model on this paper were using RF and CRF algorithm in serial,
in which each tree is build one by one sequentially or when classifying sample
each of them was given as input to each tree sequentially to get their classes.
Using parallel algorithm, for building trees or classifying samples, can
speeding up training, testing and getting classifier result.
In the domain of machine learning, an interesting new algorithm is eXtreme
Gradient Boostring (XGBoost)
\cite{chen2016xgboost}.
Using XGBoost on Wikipedia vandalism dataset maybe can increase the accuracy
of detection.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibliography}

\end{document}
