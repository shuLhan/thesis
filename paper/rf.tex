\textit{Random Forest} (RF)
is a combination of several decision tree such that each tree depends on the
value of random vector sampled independently and with equal distribution for
all trees in the forest
\cite{breiman2001random}.

There are three common paremeter in building RF.
The first parameter is the number of trees in forest ($n$),
the other two parameters are percentage of samples ($b$) and number of random
features ($m$) for building the tree.
All of the parameters are set before building each tree and their value is
constant.

Percentage of sample for training that was selected randomly usually two third
from overall samples, which left one third of them as out of bag (OOB) samples.
For the number of random features $m$, the common value is the square root or
log of all features \cite{breiman2001random}.

Procedure to build RF is as follows.
Let $S$ be a training set.
After their parameter has been set, when building each tree, take $b$ samples
randomly from $S$ without replacement (sample that get selected can be picked
again in the next iteration).
This process also know as bootstrapping.
Samples that does not included in $b$ is called out-of-bag (OOB), which can be
used to calculate the misclassification rate.
From $b$ samples, take $m$ random features, and then build the tree using $b$
samples with $m$ number of features without pruning.
Repeat the process until the $n$-tree has been built.

Classification process on RF proceed as follows.
Given test set $T$, with the same number of features with $S$.
For each sample $t$ in $T$, insert the sample $t$ into each tree and collect
their classification result.
After $n$ trees or $n$ number of classes, compute the majority class from all
tree classification.
